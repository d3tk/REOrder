run_name: longformer-base_IN1K_patch-hilbert_pemb-row-learned_cls

log_rules:
  project: $INSERT_PROJECT_NAME
  entity: $INSERT_ENTITY_NAME
  log: True
  log_interval: 20

# MODEL PARAMETERS
model_type: longformer
size: base
model:
  img_size: 224
  patch_size: 16
  in_chans: 3
  num_classes: 1000
  patch_dir: HILBERT
  pe_order: ROW
  pe_mode: learned
  

# TRAINING PARAMETERS
training:
  compile: True
  dataset:
    name: imagenet-1k
    subset: 1
    data_transforms:
      - name: Resize
        kwargs:
          size: 256
      - name: CenterCrop
        kwargs:
          size: 224
      - name: RandomHorizontalFlip
      - name: ToTensor
      - name: Normalize
        kwargs:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
    target_transforms: null
  dist: True
  shuffle: True
  seed: 42
  batch_size: 576
  num_epochs: 100
  num_workers: 4
  pin_memory: True
  drop_last: True
  replacement: False
  num_samples: null
  clip_grad: 5.0
  persistent_workers: True
  
## VALIDATION PARAMETERS
validation:
  dataset:
    name: imagenet-1k
    subset: 1
    data_transforms:
      - name: Resize
        kwargs:
          size: 256
      - name: CenterCrop
        kwargs:
          size: 224
      - name: ToTensor
      - name: Normalize
        kwargs:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
    target_transforms: null
  dist: True
  shuffle: False
  seed: ${training.seed}
  batch_size: ${training.batch_size}
  num_workers: ${training.num_workers}
  pin_memory: True
  drop_last: False
  replacement: False
  persistent_workers: False
  num_samples: null

## OPTIMIZER
optimizer:
  name: AdamW
  base_lr: 1.0e-4
  weight_decay: 0.03
  betas: [0.9, 0.999]
  grad_scale: True
  init_scale: 1024
  autocast: True
  
## SCHEDULER
scheduler:
  warmup_epochs: 5
  warmup_factor: 1
  name: cosine
  min_lr_ratio: 0.01

## SAVING CHECKPOINTS
save_every: 10

